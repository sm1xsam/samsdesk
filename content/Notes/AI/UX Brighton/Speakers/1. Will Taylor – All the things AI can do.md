As a UX researcher or designer, it’s hard to keep track of the AI technologies that we can add to our products. So this talk is a primer. It’s a visual, guided-tour of what’s available - what’s “on the menu” - to help you design amazing new experiences for users.

> Aristotle notes in On the Soul that Thales held the belief that “**the magnet has a soul in it because it moves the iron**.”

# Key Points
**Timeline of AI Research Breakthroughs**

## 1960s: ELIZA and Rule-Based AI
- **[ELIZA (1966)](https://dl.acm.org/doi/10.1145/365153.365168):** ELIZA was a simple chatbot created by Joseph Weizenbaum. It used basic patterns to "talk" like a therapist, echoing what people said to keep the conversation going. People often felt ELIZA was smarter than it was—an effect now called the **ELIZA Effect**. This reaction showed how quickly people attribute "intelligence" to machines, even if they’re just following patterns.
- **Rule-Based AI (GOFAI):** Early AI, also called "Good Old-Fashioned AI" (GOFAI), relied on **rules** defined by humans. For instance, if an AI system needed to recognise a cat, it might have rules about "pointy ears" and "whiskers." This approach worked but couldn’t scale for more complex, realistic tasks.

## 1980s: Neural Networks 
- **Backpropagation (the magic behind learning):** In the 80s, researchers rediscovered "neural networks" that could be trained to learn patterns. By tweaking a process called **backpropagation**, they made it possible to train networks on larger data sets and improve results over time.
- **What’s a Neural Network?** Think of a neural network as a simplified model of the brain, with "neurons" (nodes) connected by "synapses" (edges). They learn by adjusting connections based on the data they’re fed, making them great for image recognition and other pattern-heavy tasks.

## 1990s: AI Learns from Data
- **Shift to Data-Based AI:** AI started learning from data instead of hard-coded rules. This meant feeding it tons of examples and letting it "figure out" patterns. For example, show it enough pictures of cats and dogs, and it could start recognising them without needing specific rules.
- **Support Vector Machines (SVMs):** A powerful algorithm for recognising categories, SVMs helped in tasks like handwriting recognition. It marked a shift towards using maths and data over traditional rules.
- **Statistical Models for Language:** With **Hidden Markov Models** and other statistical tools, AI started understanding spoken language by recognising likely patterns in sounds—huge for voice recognition.

## 2000s: Big Data Changes Everything
- **Data Boom:** With the internet came a ton of [[8. Maggie Appleton – The Expanding Dark Forest and Generative AI|data]], giving AI way more to work with. This "big data" enabled better predictions and recommendations.
- **Probabilistic Models:** AI began handling uncertainty and “guessing” based on probability, useful in fields like **natural language processing** (e.g., Google search) and **computer vision** (e.g., face detection).
- **Improved Hardware:** Graphics processing units (GPUs), designed for gaming, turned out to be perfect for training large AI models. This boost in power helped AI develop faster.

## 2010s: The Deep Learning Boom
- **Deep Learning Revolution:** AI took a huge leap with deep learning, a way to train very large neural networks with many layers (hence "deep"). It turned out these networks were incredible at recognising images, sounds, and language.
- **Image Recognition with CNNs:** **Convolutional Neural Networks (CNNs)** were particularly good at picking out patterns in images. After **AlexNet** won a major image recognition contest in 2012, CNNs became the go-to for visual recognition.
- **Language Processing with RNNs and LSTMs:** For tasks like translating text, **Recurrent Neural Networks (RNNs)** became important. Their improved version, **LSTMs**, could "remember" sequences better—great for translating phrases that depend on earlier words.
- **Generative AI (GANs):** Generative Adversarial Networks (GANs) marked the start of AI creating new content, from realistic faces to artwork. GANs use two networks, one trying to create realistic images and the other acting as a critic, pushing the first network to improve.

## 2017 and Beyond: Transformers and Large Language Models
- **[Transformer Models (2017)](https://arxiv.org/abs/1706.03762):** Transformers revolutionised language processing by understanding words in context, not just sequence. Unlike RNNs, they could process whole sentences at once, leading to much more fluent AI writing.
- **GPT Series:** OpenAI’s **GPT models** (especially GPT-3 and GPT-4) demonstrated the power of transformers to generate text that reads naturally. This changed how chatbots, writing tools, and even code suggestions worked.
- **BERT and Bidirectional Models:** Google’s **BERT** model (2019) brought a new level of understanding by processing text in both directions, improving AI’s grasp of context for tasks like question-answering.
- **Scaling Up with Foundation Models:** Companies like OpenAI and Google built **large language models**, proving that bigger models (trained on huge datasets) tend to perform better across tasks.

## Present Day: Autonomy, Ethics, and AI Safety
- **Reinforcement Learning with Human Feedback (RLHF):** Today, some models use feedback from people to fine-tune their responses, making AI feel more aligned with human needs and safer to deploy.
- **AI Ethics and Transparency:** With AI’s growing power, ethical and safety concerns are top of mind. Efforts in **Explainable AI (XAI)** aim to make these complex models more understandable and transparent.
- **AI Agents:** Modern AI research explores autonomous AI that can take action based on goals. These “AI agents” are a step toward more general-purpose AI, capable of handling a broader range of tasks.